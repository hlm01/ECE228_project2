{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c381f9d8",
      "metadata": {
        "id": "c381f9d8"
      },
      "source": [
        "# attentive.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc_L6am5z7s2",
        "outputId": "60ad463e-e2a5-4f3c-979c-a29d24ea5bd9"
      },
      "id": "cc_L6am5z7s2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import GRUCell, Linear, Parameter\n",
        "\n",
        "from torch_geometric.nn import GATConv, MessagePassing, global_add_pool\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "from torch_geometric.utils import softmax\n",
        "\n",
        "\n",
        "class GATEConv(MessagePassing):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        edge_dim: int,\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        super().__init__(aggr='add', node_dim=0)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.att_l = Parameter(torch.empty(1, out_channels))\n",
        "        self.att_r = Parameter(torch.empty(1, in_channels))\n",
        "\n",
        "        self.lin1 = Linear(in_channels + edge_dim, out_channels, False)\n",
        "        self.lin2 = Linear(out_channels, out_channels, False)\n",
        "\n",
        "        self.bias = Parameter(torch.empty(out_channels))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.att_l)\n",
        "        glorot(self.att_r)\n",
        "        glorot(self.lin1.weight)\n",
        "        glorot(self.lin2.weight)\n",
        "        zeros(self.bias)\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: Tensor) -> Tensor:\n",
        "        # edge_updater_type: (x: Tensor, edge_attr: Tensor)\n",
        "        alpha = self.edge_updater(edge_index, x=x, edge_attr=edge_attr)\n",
        "\n",
        "        # propagate_type: (x: Tensor, alpha: Tensor)\n",
        "        out = self.propagate(edge_index, x=x, alpha=alpha)\n",
        "        out = out + self.bias\n",
        "        return out\n",
        "\n",
        "    def edge_update(self, x_j: Tensor, x_i: Tensor, edge_attr: Tensor,\n",
        "                    index: Tensor, ptr: OptTensor,\n",
        "                    size_i: Optional[int]) -> Tensor:\n",
        "        x_j = F.leaky_relu_(self.lin1(torch.cat([x_j, edge_attr], dim=-1)))\n",
        "        alpha_j = (x_j @ self.att_l.t()).squeeze(-1)\n",
        "        alpha_i = (x_i @ self.att_r.t()).squeeze(-1)\n",
        "        alpha = alpha_j + alpha_i\n",
        "        alpha = F.leaky_relu_(alpha)\n",
        "        alpha = softmax(alpha, index, ptr, size_i)\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "        return alpha\n",
        "\n",
        "    def message(self, x_j: Tensor, alpha: Tensor) -> Tensor:\n",
        "        return self.lin2(x_j) * alpha.unsqueeze(-1)\n",
        "\n",
        "\n",
        "class AttentiveFP(torch.nn.Module):\n",
        "    r\"\"\"The Attentive FP model for molecular representation learning from the\n",
        "    `\"Pushing the Boundaries of Molecular Representation for Drug Discovery\n",
        "    with the Graph Attention Mechanism\"\n",
        "    <https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959>`_ paper, based on\n",
        "    graph attention mechanisms.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Hidden node feature dimensionality.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        edge_dim (int): Edge feature dimensionality.\n",
        "        num_layers (int): Number of GNN layers.\n",
        "        num_timesteps (int): Number of iterative refinement steps for global\n",
        "            readout.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.0`)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        edge_dim: int,\n",
        "        num_layers: int,\n",
        "        num_timesteps: int,\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.edge_dim = edge_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.lin1 = Linear(in_channels, hidden_channels)\n",
        "\n",
        "        self.gate_conv = GATEConv(hidden_channels, hidden_channels, edge_dim,\n",
        "                                  dropout)\n",
        "        self.gru = GRUCell(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.atom_convs = torch.nn.ModuleList()\n",
        "        self.atom_grus = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers - 1):\n",
        "            conv = GATConv(hidden_channels, hidden_channels, dropout=dropout,\n",
        "                           add_self_loops=False, negative_slope=0.01)\n",
        "            self.atom_convs.append(conv)\n",
        "            self.atom_grus.append(GRUCell(hidden_channels, hidden_channels))\n",
        "\n",
        "        self.mol_conv = GATConv(hidden_channels, hidden_channels,\n",
        "                                dropout=dropout, add_self_loops=False,\n",
        "                                negative_slope=0.01)\n",
        "        self.mol_conv.explain = False  # Cannot explain global pooling.\n",
        "        self.mol_gru = GRUCell(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.lin2 = Linear(hidden_channels, out_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
        "        self.lin1.reset_parameters()\n",
        "        self.gate_conv.reset_parameters()\n",
        "        self.gru.reset_parameters()\n",
        "        for conv, gru in zip(self.atom_convs, self.atom_grus):\n",
        "            conv.reset_parameters()\n",
        "            gru.reset_parameters()\n",
        "        self.mol_conv.reset_parameters()\n",
        "        self.mol_gru.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor,\n",
        "                batch: Tensor) -> Tensor:\n",
        "        \"\"\"\"\"\"  # noqa: D419\n",
        "        # Atom Embedding:\n",
        "        x = F.leaky_relu_(self.lin1(x))\n",
        "\n",
        "        h = F.elu_(self.gate_conv(x, edge_index, edge_attr))\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        x = self.gru(h, x).relu_()\n",
        "\n",
        "        for conv, gru in zip(self.atom_convs, self.atom_grus):\n",
        "            h = conv(x, edge_index)\n",
        "            h = F.elu(h)\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            x = gru(h, x).relu()\n",
        "\n",
        "        # Molecule Embedding:\n",
        "        row = torch.arange(batch.size(0), device=batch.device)\n",
        "        edge_index = torch.stack([row, batch], dim=0)\n",
        "\n",
        "        out = global_add_pool(x, batch).relu_()\n",
        "        for t in range(self.num_timesteps):\n",
        "            h = F.elu_(self.mol_conv((x, out), edge_index))\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            out = self.mol_gru(h, out).relu_()\n",
        "\n",
        "        # Predictor:\n",
        "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "        return self.lin2(out)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}('\n",
        "                f'in_channels={self.in_channels}, '\n",
        "                f'hidden_channels={self.hidden_channels}, '\n",
        "                f'out_channels={self.out_channels}, '\n",
        "                f'edge_dim={self.edge_dim}, '\n",
        "                f'num_layers={self.num_layers}, '\n",
        "                f'num_timesteps={self.num_timesteps}'\n",
        "                f')')\n"
      ],
      "metadata": {
        "id": "AA2mdfzmzhx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb393a3-383d-4760-9eb8-48e4124f8304"
      },
      "id": "AA2mdfzmzhx8",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b6d2fb5",
      "metadata": {
        "id": "5b6d2fb5"
      },
      "source": [
        "# attentive_fp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9feef81e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9feef81e",
        "outputId": "6d293d31-b149-4be8-9522-123a9fdb76d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install rdkit\n",
        "!pip install optuna\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import os.path as osp\n",
        "from math import sqrt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from rdkit import Chem\n",
        "\n",
        "from torch_geometric.datasets import MoleculeNet\n",
        "from torch_geometric.loader import DataLoader\n",
        "#from attentive_fp import AttentiveFP\n",
        "\n",
        "\n",
        "class GenFeatures:\n",
        "    def __init__(self):\n",
        "        self.symbols = [\n",
        "            'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br',\n",
        "            'Te', 'I', 'At', 'other'\n",
        "        ]\n",
        "\n",
        "        self.hybridizations = [\n",
        "            Chem.rdchem.HybridizationType.SP,\n",
        "            Chem.rdchem.HybridizationType.SP2,\n",
        "            Chem.rdchem.HybridizationType.SP3,\n",
        "            Chem.rdchem.HybridizationType.SP3D,\n",
        "            Chem.rdchem.HybridizationType.SP3D2,\n",
        "            'other',\n",
        "        ]\n",
        "\n",
        "        self.stereos = [\n",
        "            Chem.rdchem.BondStereo.STEREONONE,\n",
        "            Chem.rdchem.BondStereo.STEREOANY,\n",
        "            Chem.rdchem.BondStereo.STEREOZ,\n",
        "            Chem.rdchem.BondStereo.STEREOE,\n",
        "        ]\n",
        "\n",
        "    def __call__(self, data):\n",
        "        # Generate AttentiveFP features according to Table 1.\n",
        "        mol = Chem.MolFromSmiles(data.smiles)\n",
        "\n",
        "        xs = []\n",
        "        for atom in mol.GetAtoms():\n",
        "            symbol = [0.] * len(self.symbols)\n",
        "            symbol[self.symbols.index(atom.GetSymbol())] = 1.\n",
        "            degree = [0.] * 6\n",
        "            degree[atom.GetDegree()] = 1.\n",
        "            formal_charge = atom.GetFormalCharge()\n",
        "            radical_electrons = atom.GetNumRadicalElectrons()\n",
        "            hybridization = [0.] * len(self.hybridizations)\n",
        "            hybridization[self.hybridizations.index(\n",
        "                atom.GetHybridization())] = 1.\n",
        "            aromaticity = 1. if atom.GetIsAromatic() else 0.\n",
        "            hydrogens = [0.] * 5\n",
        "            hydrogens[atom.GetTotalNumHs()] = 1.\n",
        "            chirality = 1. if atom.HasProp('_ChiralityPossible') else 0.\n",
        "            chirality_type = [0.] * 2\n",
        "            if atom.HasProp('_CIPCode'):\n",
        "                chirality_type[['R', 'S'].index(atom.GetProp('_CIPCode'))] = 1.\n",
        "\n",
        "            x = torch.tensor(symbol + degree + [formal_charge] +\n",
        "                             [radical_electrons] + hybridization +\n",
        "                             [aromaticity] + hydrogens + [chirality] +\n",
        "                             chirality_type)\n",
        "            xs.append(x)\n",
        "\n",
        "        data.x = torch.stack(xs, dim=0)\n",
        "\n",
        "        edge_indices = []\n",
        "        edge_attrs = []\n",
        "        for bond in mol.GetBonds():\n",
        "            edge_indices += [[bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()]]\n",
        "            edge_indices += [[bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()]]\n",
        "\n",
        "            bond_type = bond.GetBondType()\n",
        "            single = 1. if bond_type == Chem.rdchem.BondType.SINGLE else 0.\n",
        "            double = 1. if bond_type == Chem.rdchem.BondType.DOUBLE else 0.\n",
        "            triple = 1. if bond_type == Chem.rdchem.BondType.TRIPLE else 0.\n",
        "            aromatic = 1. if bond_type == Chem.rdchem.BondType.AROMATIC else 0.\n",
        "            conjugation = 1. if bond.GetIsConjugated() else 0.\n",
        "            ring = 1. if bond.IsInRing() else 0.\n",
        "            stereo = [0.] * 4\n",
        "            stereo[self.stereos.index(bond.GetStereo())] = 1.\n",
        "\n",
        "            edge_attr = torch.tensor(\n",
        "                [single, double, triple, aromatic, conjugation, ring] + stereo)\n",
        "\n",
        "            edge_attrs += [edge_attr, edge_attr]\n",
        "\n",
        "        if len(edge_attrs) == 0:\n",
        "            data.edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
        "            data.edge_attr = torch.zeros((0, 10), dtype=torch.float)\n",
        "        else:\n",
        "            data.edge_index = torch.tensor(edge_indices).t().contiguous()\n",
        "            data.edge_attr = torch.stack(edge_attrs, dim=0)\n",
        "\n",
        "        return data\n",
        "\n",
        "def train():\n",
        "    total_loss = total_examples = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = F.mse_loss(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        total_examples += data.num_graphs\n",
        "    return sqrt(total_loss / total_examples)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    mse = []\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        mse.append(F.mse_loss(out, data.y, reduction='none').cpu())\n",
        "    return float(torch.cat(mse, dim=0).mean().sqrt())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(20)\n",
        "#path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'AFP_Mol')\n",
        "\n",
        "path = '/content/drive/MyDrive/my_project/data/AFP_Mol'\n",
        "dataset = MoleculeNet(path, name='ESOL', pre_transform=GenFeatures()).shuffle()\n",
        "\n",
        "N = len(dataset) // 10\n",
        "val_dataset = dataset[:N]\n",
        "test_dataset = dataset[N:2 * N]\n",
        "train_dataset = dataset[2 * N:]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=200)\n",
        "test_loader = DataLoader(test_dataset, batch_size=200)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = AttentiveFP(in_channels=39, hidden_channels=200, out_channels=1,\n",
        "                    edge_dim=10, num_layers=3, num_timesteps=2,\n",
        "                    dropout=0.2).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=10**-2.5,\n",
        "                             weight_decay=10**-5)\n",
        "\n",
        "for epoch in range(1, 401):\n",
        "    train_rmse = train()\n",
        "    val_rmse = test(val_loader)\n",
        "    test_rmse = test(test_loader)\n",
        "    if(epoch%10==0):\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} Val: {val_rmse:.4f} '\n",
        "          f'Test: {test_rmse:.4f}')\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    set_seed(21)  # Ensures reproducibility\n",
        "\n",
        "    # Suggest hyperparameters\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
        "    learning_rate = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
        "\n",
        "    # Define model and optimizer with trial parameters\n",
        "    model = AttentiveFP(\n",
        "        in_channels=39,\n",
        "        hidden_channels=200,\n",
        "        out_channels=1,\n",
        "        edge_dim=10,\n",
        "        num_layers=num_layers,\n",
        "        num_timesteps=2,\n",
        "        dropout=0.2\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # Early stopping\n",
        "    best_val = float(\"inf\")\n",
        "    best_model_state = None\n",
        "    patience = 20\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(1, 201):\n",
        "        train_rmse = train()\n",
        "        val_rmse = test(test_loader)\n",
        "\n",
        "        if val_rmse < best_val:\n",
        "            best_val = val_rmse\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                break\n",
        "\n",
        "    # Save best model for this trial\n",
        "    trial.set_user_attr(\"best_model_state_dict\", best_model_state)\n",
        "\n",
        "    return best_val\n",
        "\n",
        "# Run study\n",
        "#study = optuna.create_study(direction=\"minimize\")\n",
        "#study.optimize(objective, n_trials=25)\n",
        "\n",
        "# Report best trial\n",
        "#best_trial = study.best_trial\n",
        "#print(\"Best Trial:\")\n",
        "#print(f\"  Value: {best_trial.value:.4f}\")\n",
        "#for k, v in best_trial.params.items():\n",
        "#    print(f\"  {k}: {v}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BAd4goU08Fz",
        "outputId": "4acdff6b-b1fd-4964-ceed-8616fc40de51"
      },
      "id": "5BAd4goU08Fz",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, Loss: 1.4272 Val: 1.2017 Test: 1.2530\n",
            "Epoch: 020, Loss: 0.9246 Val: 0.9110 Test: 0.8229\n",
            "Epoch: 030, Loss: 0.7678 Val: 0.9296 Test: 0.7024\n",
            "Epoch: 040, Loss: 0.7234 Val: 0.7320 Test: 0.6771\n",
            "Epoch: 050, Loss: 0.6612 Val: 0.7158 Test: 0.6164\n",
            "Epoch: 060, Loss: 0.6225 Val: 0.7269 Test: 0.6348\n",
            "Epoch: 070, Loss: 0.5677 Val: 0.6671 Test: 0.5785\n",
            "Epoch: 080, Loss: 0.5583 Val: 0.6009 Test: 0.6250\n",
            "Epoch: 090, Loss: 0.5188 Val: 0.6260 Test: 0.6107\n",
            "Epoch: 100, Loss: 0.5138 Val: 0.6384 Test: 0.5169\n",
            "Epoch: 110, Loss: 0.4906 Val: 0.6150 Test: 0.5559\n",
            "Epoch: 120, Loss: 0.4806 Val: 0.5695 Test: 0.5838\n",
            "Epoch: 130, Loss: 0.4416 Val: 0.5943 Test: 0.5016\n",
            "Epoch: 140, Loss: 0.4220 Val: 0.5794 Test: 0.5871\n",
            "Epoch: 150, Loss: 0.4479 Val: 0.6156 Test: 0.6001\n",
            "Epoch: 160, Loss: 0.4307 Val: 0.6364 Test: 0.5886\n",
            "Epoch: 170, Loss: 0.3988 Val: 0.5525 Test: 0.5320\n",
            "Epoch: 180, Loss: 0.4280 Val: 0.5706 Test: 0.5224\n",
            "Epoch: 190, Loss: 0.3834 Val: 0.5600 Test: 0.5323\n",
            "Epoch: 200, Loss: 0.4023 Val: 0.5772 Test: 0.6184\n",
            "Epoch: 210, Loss: 0.3910 Val: 0.5502 Test: 0.5801\n",
            "Epoch: 220, Loss: 0.3620 Val: 0.5153 Test: 0.5684\n",
            "Epoch: 230, Loss: 0.3757 Val: 0.5712 Test: 0.4777\n",
            "Epoch: 240, Loss: 0.3818 Val: 0.5378 Test: 0.5585\n",
            "Epoch: 250, Loss: 0.3754 Val: 0.5544 Test: 0.5578\n",
            "Epoch: 260, Loss: 0.3426 Val: 0.5478 Test: 0.5405\n",
            "Epoch: 270, Loss: 0.3621 Val: 0.6094 Test: 0.5154\n",
            "Epoch: 280, Loss: 0.3452 Val: 0.5377 Test: 0.5215\n",
            "Epoch: 290, Loss: 0.3443 Val: 0.5005 Test: 0.5273\n",
            "Epoch: 300, Loss: 0.3464 Val: 0.5671 Test: 0.5225\n",
            "Epoch: 310, Loss: 0.3508 Val: 0.5084 Test: 0.5012\n",
            "Epoch: 320, Loss: 0.3165 Val: 0.5302 Test: 0.5165\n",
            "Epoch: 330, Loss: 0.3320 Val: 0.5102 Test: 0.5045\n",
            "Epoch: 340, Loss: 0.3432 Val: 0.5442 Test: 0.4622\n",
            "Epoch: 350, Loss: 0.3093 Val: 0.5743 Test: 0.4902\n",
            "Epoch: 360, Loss: 0.3325 Val: 0.5412 Test: 0.5511\n",
            "Epoch: 370, Loss: 0.3110 Val: 0.5354 Test: 0.5200\n",
            "Epoch: 380, Loss: 0.3071 Val: 0.5137 Test: 0.5129\n",
            "Epoch: 390, Loss: 0.3466 Val: 0.5551 Test: 0.4986\n",
            "Epoch: 400, Loss: 0.3067 Val: 0.5324 Test: 0.4839\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}