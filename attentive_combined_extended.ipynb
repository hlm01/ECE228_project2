{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c381f9d8",
   "metadata": {
    "id": "c381f9d8"
   },
   "source": [
    "# attentive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "AA2mdfzmzhx8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AA2mdfzmzhx8",
    "outputId": "0fb393a3-383d-4760-9eb8-48e4124f8304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch_geometric in /home/osathe/.local/lib/python3.11/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.9.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch_geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import GRUCell, Linear, Parameter\n",
    "\n",
    "from torch_geometric.nn import GATConv, MessagePassing, global_add_pool\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "\n",
    "class GATEConv(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        edge_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__(aggr='add', node_dim=0)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.att_l = Parameter(torch.empty(1, out_channels))\n",
    "        self.att_r = Parameter(torch.empty(1, in_channels))\n",
    "\n",
    "        self.lin1 = Linear(in_channels + edge_dim, out_channels, False)\n",
    "        self.lin2 = Linear(out_channels, out_channels, False)\n",
    "\n",
    "        self.bias = Parameter(torch.empty(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.att_l)\n",
    "        glorot(self.att_r)\n",
    "        glorot(self.lin1.weight)\n",
    "        glorot(self.lin2.weight)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: Tensor) -> Tensor:\n",
    "        # edge_updater_type: (x: Tensor, edge_attr: Tensor)\n",
    "        alpha = self.edge_updater(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "        # propagate_type: (x: Tensor, alpha: Tensor)\n",
    "        out = self.propagate(edge_index, x=x, alpha=alpha)\n",
    "        out = out + self.bias\n",
    "        return out\n",
    "\n",
    "    def edge_update(self, x_j: Tensor, x_i: Tensor, edge_attr: Tensor,\n",
    "                    index: Tensor, ptr: OptTensor,\n",
    "                    size_i: Optional[int]) -> Tensor:\n",
    "        x_j = F.leaky_relu_(self.lin1(torch.cat([x_j, edge_attr], dim=-1)))\n",
    "        alpha_j = (x_j @ self.att_l.t()).squeeze(-1)\n",
    "        alpha_i = (x_i @ self.att_r.t()).squeeze(-1)\n",
    "        alpha = alpha_j + alpha_i\n",
    "        alpha = F.leaky_relu_(alpha)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return alpha\n",
    "\n",
    "    def message(self, x_j: Tensor, alpha: Tensor) -> Tensor:\n",
    "        return self.lin2(x_j) * alpha.unsqueeze(-1)\n",
    "\n",
    "\n",
    "class AttentiveFP(torch.nn.Module):\n",
    "    r\"\"\"The Attentive FP model for molecular representation learning from the\n",
    "    `\"Pushing the Boundaries of Molecular Representation for Drug Discovery\n",
    "    with the Graph Attention Mechanism\"\n",
    "    <https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959>`_ paper, based on\n",
    "    graph attention mechanisms.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        hidden_channels (int): Hidden node feature dimensionality.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        edge_dim (int): Edge feature dimensionality.\n",
    "        num_layers (int): Number of GNN layers.\n",
    "        num_timesteps (int): Number of iterative refinement steps for global\n",
    "            readout.\n",
    "        dropout (float, optional): Dropout probability. (default: :obj:`0.0`)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        edge_dim: int,\n",
    "        num_layers: int,\n",
    "        num_timesteps: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.edge_dim = edge_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin1 = Linear(in_channels, hidden_channels)\n",
    "\n",
    "        self.gate_conv = GATEConv(hidden_channels, hidden_channels, edge_dim,\n",
    "                                  dropout)\n",
    "        self.gru = GRUCell(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.atom_convs = torch.nn.ModuleList()\n",
    "        self.atom_grus = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            conv = GATConv(hidden_channels, hidden_channels, dropout=dropout,\n",
    "                           add_self_loops=False, negative_slope=0.01)\n",
    "            self.atom_convs.append(conv)\n",
    "            self.atom_grus.append(GRUCell(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.mol_conv = GATConv(hidden_channels, hidden_channels,\n",
    "                                dropout=dropout, add_self_loops=False,\n",
    "                                negative_slope=0.01)\n",
    "        self.mol_conv.explain = False  # Cannot explain global pooling.\n",
    "        self.mol_gru = GRUCell(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.lin2 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        self.lin1.reset_parameters()\n",
    "        self.gate_conv.reset_parameters()\n",
    "        self.gru.reset_parameters()\n",
    "        for conv, gru in zip(self.atom_convs, self.atom_grus):\n",
    "            conv.reset_parameters()\n",
    "            gru.reset_parameters()\n",
    "        self.mol_conv.reset_parameters()\n",
    "        self.mol_gru.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor,\n",
    "                batch: Tensor) -> Tensor:\n",
    "        \"\"\"\"\"\"  # noqa: D419\n",
    "        # Atom Embedding:\n",
    "        x = F.leaky_relu_(self.lin1(x))\n",
    "\n",
    "        h = F.elu_(self.gate_conv(x, edge_index, edge_attr))\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        x = self.gru(h, x).relu_()\n",
    "\n",
    "        for conv, gru in zip(self.atom_convs, self.atom_grus):\n",
    "            h = conv(x, edge_index)\n",
    "            h = F.elu(h)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            x = gru(h, x).relu()\n",
    "\n",
    "        # Molecule Embedding:\n",
    "        row = torch.arange(batch.size(0), device=batch.device)\n",
    "        edge_index = torch.stack([row, batch], dim=0)\n",
    "\n",
    "        out = global_add_pool(x, batch).relu_()\n",
    "        for t in range(self.num_timesteps):\n",
    "            h = F.elu_(self.mol_conv((x, out), edge_index))\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            out = self.mol_gru(h, out).relu_()\n",
    "\n",
    "        # Predictor:\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        return self.lin2(out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}('\n",
    "                f'in_channels={self.in_channels}, '\n",
    "                f'hidden_channels={self.hidden_channels}, '\n",
    "                f'out_channels={self.out_channels}, '\n",
    "                f'edge_dim={self.edge_dim}, '\n",
    "                f'num_layers={self.num_layers}, '\n",
    "                f'num_timesteps={self.num_timesteps}'\n",
    "                f')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d2fb5",
   "metadata": {
    "id": "5b6d2fb5"
   },
   "source": [
    "# attentive_fp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9feef81e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9feef81e",
    "outputId": "6d293d31-b149-4be8-9522-123a9fdb76d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:113: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "<>:113: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rdkit in /home/osathe/.local/lib/python3.11/site-packages (2025.3.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from rdkit) (10.4.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optuna in /home/osathe/.local/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: colorlog in /home/osathe/.local/lib/python3.11/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (24.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna) (2.0.29)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1493/2891615107.py:113: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "  + [bond_length, bond_dir]\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit\n",
    "!pip install optuna\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import os.path as osp\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "#from attentive_fp import AttentiveFP\n",
    "\n",
    "\n",
    "class GenFeatures:\n",
    "    def __init__(self):\n",
    "        self.symbols = [\n",
    "            'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br',\n",
    "            'Te', 'I', 'At', 'other'\n",
    "        ]\n",
    "\n",
    "        self.hybridizations = [\n",
    "            Chem.rdchem.HybridizationType.SP,\n",
    "            Chem.rdchem.HybridizationType.SP2,\n",
    "            Chem.rdchem.HybridizationType.SP3,\n",
    "            Chem.rdchem.HybridizationType.SP3D,\n",
    "            Chem.rdchem.HybridizationType.SP3D2,\n",
    "            'other',\n",
    "        ]\n",
    "\n",
    "        self.stereos = [\n",
    "            Chem.rdchem.BondStereo.STEREONONE,\n",
    "            Chem.rdchem.BondStereo.STEREOANY,\n",
    "            Chem.rdchem.BondStereo.STEREOZ,\n",
    "            Chem.rdchem.BondStereo.STEREOE,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Generate AttentiveFP features according to Table 1.\n",
    "        mol = Chem.MolFromSmiles(data.smiles)\n",
    "\n",
    "        xs = []\n",
    "        from rdkit.Chem import Descriptors\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Generate AttentiveFP features according to Table 1.\n",
    "        mol = Chem.MolFromSmiles(data.smiles)\n",
    "\n",
    "        xs = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            symbol = [0.] * len(self.symbols)\n",
    "            symbol[self.symbols.index(atom.GetSymbol())] = 1.\n",
    "            degree = [0.] * 6\n",
    "            degree[atom.GetDegree()] = 1.\n",
    "            formal_charge = atom.GetFormalCharge()\n",
    "            radical_electrons = atom.GetNumRadicalElectrons()\n",
    "            hybridization = [0.] * len(self.hybridizations)\n",
    "            hybridization[self.hybridizations.index(\n",
    "                atom.GetHybridization())] = 1.\n",
    "            aromaticity = 1. if atom.GetIsAromatic() else 0.\n",
    "            hydrogens = [0.] * 5\n",
    "            hydrogens[atom.GetTotalNumHs()] = 1.\n",
    "            chirality = 1. if atom.HasProp('_ChiralityPossible') else 0.\n",
    "            chirality_type = [0.] * 2\n",
    "            is_in_ring = 1. if atom.IsInRing() else 0.\n",
    "            mass = atom.GetMass()\n",
    "            is_donor = 1. if rdMolDescriptors.CalcNumHBD(mol) > 0 else 0.\n",
    "            is_acceptor = 1. if rdMolDescriptors.CalcNumHBA(mol) > 0 else 0.\n",
    "            if atom.HasProp('_CIPCode'):\n",
    "                chirality_type[['R', 'S'].index(atom.GetProp('_CIPCode'))] = 1.\n",
    "\n",
    "            # Append extra atom features\n",
    "            x = torch.cat((x, torch.tensor([is_in_ring, mass, is_donor, is_acceptor])))\n",
    "\n",
    "            x = torch.tensor(symbol + degree + [formal_charge] +\n",
    "                             [radical_electrons] + hybridization +\n",
    "                             [aromaticity] + hydrogens + [chirality] +\n",
    "                             chirality_type)\n",
    "            xs.append(x)\n",
    "\n",
    "        data.x = torch.stack(xs, dim=0)\n",
    "\n",
    "        edge_indices = []\n",
    "        edge_attrs = []\n",
    "        for bond in mol.GetBonds():\n",
    "            bond_dir = 0.\n",
    "            if bond.GetBondDir() in [BondDir.ENDUPRIGHT, BondDir.ENDDOWNRIGHT]:\n",
    "                bond_dir = 1.\n",
    "\n",
    "            if conf is not None:\n",
    "                idx1, idx2 = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                pos1 = conf.GetAtomPosition(idx1)\n",
    "                pos2 = conf.GetAtomPosition(idx2)\n",
    "                bond_length = float((pos1 - pos2).Length())\n",
    "            else:\n",
    "                bond_length = 0.\n",
    "            edge_indices += [[bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()]]\n",
    "            edge_indices += [[bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()]]\n",
    "\n",
    "            bond_type = bond.GetBondType()\n",
    "            single = 1. if bond_type == Chem.rdchem.BondType.SINGLE else 0.\n",
    "            double = 1. if bond_type == Chem.rdchem.BondType.DOUBLE else 0.\n",
    "            triple = 1. if bond_type == Chem.rdchem.BondType.TRIPLE else 0.\n",
    "            aromatic = 1. if bond_type == Chem.rdchem.BondType.AROMATIC else 0.\n",
    "            conjugation = 1. if bond.GetIsConjugated() else 0.\n",
    "            ring = 1. if bond.IsInRing() else 0.\n",
    "            stereo = [0.] * 4\n",
    "            stereo[self.stereos.index(bond.GetStereo())] = 1.\n",
    "\n",
    "            edge_attr = torch.tensor(\n",
    "                + [bond_length, bond_dir]\n",
    "                [single, double, triple, aromatic, conjugation, ring] + stereo)\n",
    "\n",
    "            edge_attrs += [edge_attr, edge_attr]\n",
    "\n",
    "        if len(edge_attrs) == 0:\n",
    "            data.edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "            data.edge_attr = torch.zeros((0, 10), dtype=torch.float)\n",
    "        else:\n",
    "            data.edge_index = torch.tensor(edge_indices).t().contiguous()\n",
    "            data.edge_attr = torch.stack(edge_attrs, dim=0)\n",
    "\n",
    "        return data\n",
    "\n",
    "def train():\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = F.mse_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "    return sqrt(total_loss / total_examples)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    mse = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        mse.append(F.mse_loss(out, data.y, reduction='none').cpu())\n",
    "    return float(torch.cat(mse, dim=0).mean().sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5BAd4goU08Fz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BAd4goU08Fz",
    "outputId": "4acdff6b-b1fd-4964-ceed-8616fc40de51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 1.2782 Val: 1.2202 Test: 1.2607\n",
      "Epoch: 020, Loss: 0.8653 Val: 0.9516 Test: 0.8233\n",
      "Epoch: 030, Loss: 0.7332 Val: 0.7329 Test: 0.6754\n",
      "Epoch: 040, Loss: 0.6479 Val: 0.7001 Test: 0.6187\n",
      "Epoch: 050, Loss: 0.6181 Val: 0.6786 Test: 0.6157\n",
      "Epoch: 060, Loss: 0.6154 Val: 0.6392 Test: 0.6436\n",
      "Epoch: 070, Loss: 0.5648 Val: 0.7107 Test: 0.5936\n",
      "Epoch: 080, Loss: 0.5834 Val: 0.6592 Test: 0.6016\n",
      "Epoch: 090, Loss: 0.5112 Val: 0.6321 Test: 0.5699\n",
      "Epoch: 100, Loss: 0.5256 Val: 0.5682 Test: 0.5859\n",
      "Epoch: 110, Loss: 0.4933 Val: 0.6473 Test: 0.5693\n",
      "Epoch: 120, Loss: 0.4890 Val: 0.5544 Test: 0.5770\n",
      "Epoch: 130, Loss: 0.4762 Val: 0.6074 Test: 0.5346\n",
      "Epoch: 140, Loss: 0.4440 Val: 0.5525 Test: 0.5428\n",
      "Epoch: 150, Loss: 0.4805 Val: 0.6095 Test: 0.5420\n",
      "Epoch: 160, Loss: 0.4340 Val: 0.5581 Test: 0.5406\n",
      "Epoch: 170, Loss: 0.4189 Val: 0.5875 Test: 0.5470\n",
      "Epoch: 180, Loss: 0.4025 Val: 0.5627 Test: 0.5200\n",
      "Epoch: 190, Loss: 0.3996 Val: 0.6139 Test: 0.5484\n",
      "Epoch: 200, Loss: 0.3763 Val: 0.5816 Test: 0.6052\n",
      "Epoch: 210, Loss: 0.3851 Val: 0.5814 Test: 0.5517\n",
      "Epoch: 220, Loss: 0.4213 Val: 0.6838 Test: 0.6014\n",
      "Epoch: 230, Loss: 0.3575 Val: 0.5627 Test: 0.5316\n",
      "Epoch: 240, Loss: 0.3605 Val: 0.6854 Test: 0.5152\n",
      "Epoch: 250, Loss: 0.3664 Val: 0.6147 Test: 0.5165\n",
      "Epoch: 260, Loss: 0.3328 Val: 0.5763 Test: 0.5491\n",
      "Epoch: 270, Loss: 0.3914 Val: 0.6049 Test: 0.5704\n",
      "Epoch: 280, Loss: 0.3388 Val: 0.5813 Test: 0.5428\n",
      "Epoch: 290, Loss: 0.3494 Val: 0.5876 Test: 0.5617\n",
      "Epoch: 300, Loss: 0.3596 Val: 0.5857 Test: 0.5839\n",
      "Epoch: 310, Loss: 0.3236 Val: 0.5752 Test: 0.5718\n",
      "Epoch: 320, Loss: 0.3147 Val: 0.5587 Test: 0.5693\n",
      "Epoch: 330, Loss: 0.3205 Val: 0.6326 Test: 0.5765\n",
      "Epoch: 340, Loss: 0.3174 Val: 0.6372 Test: 0.5643\n",
      "Epoch: 350, Loss: 0.3318 Val: 0.6134 Test: 0.5264\n",
      "Epoch: 360, Loss: 0.3153 Val: 0.6099 Test: 0.5641\n",
      "Epoch: 370, Loss: 0.3022 Val: 0.5448 Test: 0.5045\n",
      "Epoch: 380, Loss: 0.3086 Val: 0.5946 Test: 0.5528\n",
      "Epoch: 390, Loss: 0.3171 Val: 0.6139 Test: 0.6185\n",
      "Epoch: 400, Loss: 0.3088 Val: 0.5927 Test: 0.5403\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import os.path as osp\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.rdchem import BondDir\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Compute molecule-level info once\n",
    "#mol = Chem.AddHs(mol)\n",
    "#AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "#conf = mol.GetConformer() if mol.GetNumConformers() > 0 else None\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(20)\n",
    "\n",
    "# Use current working directory in Jupyter\n",
    "notebook_dir = osp.dirname(osp.abspath(''))\n",
    "path = osp.abspath('../data/AFP_Mol')\n",
    "\n",
    "#path = '/content/drive/MyDrive/my_project/data/AFP_Mol'\n",
    "dataset = MoleculeNet(path, name='ESOL', pre_transform=GenFeatures()).shuffle()\n",
    "\n",
    "N = len(dataset) // 10\n",
    "val_dataset = dataset[:N]\n",
    "test_dataset = dataset[N:2 * N]\n",
    "train_dataset = dataset[2 * N:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=200)\n",
    "test_loader = DataLoader(test_dataset, batch_size=200)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AttentiveFP(in_channels=39, hidden_channels=200, out_channels=1,\n",
    "                    edge_dim=10, num_layers=3, num_timesteps=2,\n",
    "                    dropout=0.2).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=10**-2.5,\n",
    "                             weight_decay=10**-5)\n",
    "\n",
    "for epoch in range(1, 401):\n",
    "    train_rmse = train()\n",
    "    val_rmse = test(val_loader)\n",
    "    test_rmse = test(test_loader)\n",
    "    if(epoch%10==0):\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} Val: {val_rmse:.4f} '\n",
    "          f'Test: {test_rmse:.4f}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    set_seed(21)  # Ensures reproducibility\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "\n",
    "    # Define model and optimizer with trial parameters\n",
    "    model = AttentiveFP(\n",
    "        in_channels=39,\n",
    "        hidden_channels=200,\n",
    "        out_channels=1,\n",
    "        edge_dim=10,\n",
    "        num_layers=num_layers,\n",
    "        num_timesteps=2,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    best_val = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = 20\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        train_rmse = train()\n",
    "        val_rmse = test(test_loader)\n",
    "\n",
    "        if val_rmse < best_val:\n",
    "            best_val = val_rmse\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Save best model for this trial\n",
    "    trial.set_user_attr(\"best_model_state_dict\", best_model_state)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "# Run study\n",
    "#study = optuna.create_study(direction=\"minimize\")\n",
    "#study.optimize(objective, n_trials=25)\n",
    "\n",
    "# Report best trial\n",
    "#best_trial = study.best_trial\n",
    "#print(\"Best Trial:\")\n",
    "#print(f\"  Value: {best_trial.value:.4f}\")\n",
    "#for k, v in best_trial.params.items():\n",
    "#    print(f\"  {k}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9633c-d4c2-4e62-8993-102424fdfad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
