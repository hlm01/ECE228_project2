{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpEBEqWRJvi-"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric rdkit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4IS2YXmdSM-"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import GRUCell, Linear, Parameter\n",
        "\n",
        "from torch_geometric.nn import GATConv, MessagePassing, global_add_pool\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "from torch_geometric.utils import softmax\n",
        "\n",
        "\n",
        "class GATEConv(MessagePassing):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        edge_dim: int,\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        super().__init__(aggr=\"add\", node_dim=0)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.att_l = Parameter(torch.empty(1, out_channels))\n",
        "        self.att_r = Parameter(torch.empty(1, in_channels))\n",
        "\n",
        "        self.lin1 = Linear(in_channels + edge_dim, out_channels, False)\n",
        "        self.lin2 = Linear(out_channels, out_channels, False)\n",
        "\n",
        "        self.bias = Parameter(torch.empty(out_channels))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.att_l)\n",
        "        glorot(self.att_r)\n",
        "        glorot(self.lin1.weight)\n",
        "        glorot(self.lin2.weight)\n",
        "        zeros(self.bias)\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: Tensor) -> Tensor:\n",
        "        # edge_updater_type: (x: Tensor, edge_attr: Tensor)\n",
        "        alpha = self.edge_updater(edge_index, x=x, edge_attr=edge_attr)\n",
        "\n",
        "        # propagate_type: (x: Tensor, alpha: Tensor)\n",
        "        out = self.propagate(edge_index, x=x, alpha=alpha)\n",
        "        out = out + self.bias\n",
        "        return out\n",
        "\n",
        "    def edge_update(\n",
        "        self,\n",
        "        x_j: Tensor,\n",
        "        x_i: Tensor,\n",
        "        edge_attr: Tensor,\n",
        "        index: Tensor,\n",
        "        ptr: OptTensor,\n",
        "        size_i: Optional[int],\n",
        "    ) -> Tensor:\n",
        "        x_j = F.leaky_relu_(self.lin1(torch.cat([x_j, edge_attr], dim=-1)))\n",
        "        alpha_j = (x_j @ self.att_l.t()).squeeze(-1)\n",
        "        alpha_i = (x_i @ self.att_r.t()).squeeze(-1)\n",
        "        alpha = alpha_j + alpha_i\n",
        "        alpha = F.leaky_relu_(alpha)\n",
        "        alpha = softmax(alpha, index, ptr, size_i)\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "        return alpha\n",
        "\n",
        "    def message(self, x_j: Tensor, alpha: Tensor) -> Tensor:\n",
        "        return self.lin2(x_j) * alpha.unsqueeze(-1)\n",
        "\n",
        "\n",
        "class AttentiveFP(torch.nn.Module):\n",
        "    r\"\"\"The Attentive FP model for molecular representation learning from the\n",
        "    `\"Pushing the Boundaries of Molecular Representation for Drug Discovery\n",
        "    with the Graph Attention Mechanism\"\n",
        "    <https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959>`_ paper, based on\n",
        "    graph attention mechanisms.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Hidden node feature dimensionality.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        edge_dim (int): Edge feature dimensionality.\n",
        "        num_layers (int): Number of GNN layers.\n",
        "        num_timesteps (int): Number of iterative refinement steps for global\n",
        "            readout.\n",
        "        dropout (float, optional): Dropout probability. (default: :obj:`0.0`)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        edge_dim: int,\n",
        "        num_layers: int,\n",
        "        num_timesteps: int,\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.edge_dim = edge_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.lin1 = Linear(in_channels, hidden_channels)\n",
        "\n",
        "        self.gate_conv = GATEConv(hidden_channels, hidden_channels, edge_dim, dropout)\n",
        "        self.gru = GRUCell(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.atom_convs = torch.nn.ModuleList()\n",
        "        self.atom_grus = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers - 1):\n",
        "            conv = GATConv(\n",
        "                hidden_channels,\n",
        "                hidden_channels,\n",
        "                dropout=dropout,\n",
        "                add_self_loops=False,\n",
        "                negative_slope=0.01,\n",
        "            )\n",
        "            self.atom_convs.append(conv)\n",
        "            self.atom_grus.append(GRUCell(hidden_channels, hidden_channels))\n",
        "\n",
        "        self.mol_conv = GATConv(\n",
        "            hidden_channels,\n",
        "            hidden_channels,\n",
        "            dropout=dropout,\n",
        "            add_self_loops=False,\n",
        "            negative_slope=0.01,\n",
        "        )\n",
        "        self.mol_conv.explain = False  # Cannot explain global pooling.\n",
        "        self.mol_gru = GRUCell(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.lin2 = Linear(hidden_channels, out_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
        "        self.lin1.reset_parameters()\n",
        "        self.gate_conv.reset_parameters()\n",
        "        self.gru.reset_parameters()\n",
        "        for conv, gru in zip(self.atom_convs, self.atom_grus):\n",
        "            conv.reset_parameters()\n",
        "            gru.reset_parameters()\n",
        "        self.mol_conv.reset_parameters()\n",
        "        self.mol_gru.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "\n",
        "    def forward(\n",
        "        self, x: Tensor, edge_index: Tensor, edge_attr: Tensor, batch: Tensor\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\"\"\"  # noqa: D419\n",
        "        # Atom Embedding:\n",
        "        x = F.leaky_relu_(self.lin1(x))\n",
        "\n",
        "        h = F.elu_(self.gate_conv(x, edge_index, edge_attr))\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        x = self.gru(h, x).relu_()\n",
        "\n",
        "        for conv, gru in zip(self.atom_convs, self.atom_grus):\n",
        "            # print(f\"Before: {h.shape}\")\n",
        "            h = conv(x, edge_index)\n",
        "            # print(f\"after: {h.shape}\")\n",
        "            h = F.elu(h)\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            x = gru(h, x).relu()\n",
        "\n",
        "        # Molecule Embedding:\n",
        "        row = torch.arange(batch.size(0), device=batch.device)\n",
        "        edge_index = torch.stack([row, batch], dim=0)\n",
        "\n",
        "        out = global_add_pool(x, batch).relu_()\n",
        "        for t in range(self.num_timesteps):\n",
        "            h = F.elu_(self.mol_conv((x, out), edge_index))\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            out = self.mol_gru(h, out).relu_()\n",
        "\n",
        "        # Predictor:\n",
        "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "        return self.lin2(out)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (\n",
        "            f\"{self.__class__.__name__}(\"\n",
        "            f\"in_channels={self.in_channels}, \"\n",
        "            f\"hidden_channels={self.hidden_channels}, \"\n",
        "            f\"out_channels={self.out_channels}, \"\n",
        "            f\"edge_dim={self.edge_dim}, \"\n",
        "            f\"num_layers={self.num_layers}, \"\n",
        "            f\"num_timesteps={self.num_timesteps}\"\n",
        "            f\")\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Joqq-uNqJK-W"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "from math import sqrt\n",
        "\n",
        "from rdkit import Chem\n",
        "\n",
        "from torch_geometric.datasets import MoleculeNet\n",
        "from torch_geometric.loader import DataLoader\n",
        "from tqdm import trange\n",
        "import sys\n",
        "\n",
        "\n",
        "class GenFeatures:\n",
        "    def __init__(self):\n",
        "        self.symbols = [\n",
        "            \"B\",\n",
        "            \"C\",\n",
        "            \"N\",\n",
        "            \"O\",\n",
        "            \"F\",\n",
        "            \"Si\",\n",
        "            \"P\",\n",
        "            \"S\",\n",
        "            \"Cl\",\n",
        "            \"As\",\n",
        "            \"Se\",\n",
        "            \"Br\",\n",
        "            \"Te\",\n",
        "            \"I\",\n",
        "            \"At\",\n",
        "            \"other\",\n",
        "        ]\n",
        "\n",
        "        self.hybridizations = [\n",
        "            Chem.rdchem.HybridizationType.SP,\n",
        "            Chem.rdchem.HybridizationType.SP2,\n",
        "            Chem.rdchem.HybridizationType.SP3,\n",
        "            Chem.rdchem.HybridizationType.SP3D,\n",
        "            Chem.rdchem.HybridizationType.SP3D2,\n",
        "            \"other\",\n",
        "        ]\n",
        "\n",
        "        self.stereos = [\n",
        "            Chem.rdchem.BondStereo.STEREONONE,\n",
        "            Chem.rdchem.BondStereo.STEREOANY,\n",
        "            Chem.rdchem.BondStereo.STEREOZ,\n",
        "            Chem.rdchem.BondStereo.STEREOE,\n",
        "        ]\n",
        "\n",
        "    def __call__(self, data):\n",
        "        # Generate AttentiveFP features according to Table 1.\n",
        "        mol = Chem.MolFromSmiles(data.smiles)\n",
        "\n",
        "        xs = []\n",
        "        for atom in mol.GetAtoms():\n",
        "            symbol = [0.0] * len(self.symbols)\n",
        "            symbol[self.symbols.index(atom.GetSymbol())] = 1.0\n",
        "            degree = [0.0] * 6\n",
        "            degree[atom.GetDegree()] = 1.0\n",
        "            formal_charge = atom.GetFormalCharge()\n",
        "            radical_electrons = atom.GetNumRadicalElectrons()\n",
        "            hybridization = [0.0] * len(self.hybridizations)\n",
        "            hybridization[self.hybridizations.index(atom.GetHybridization())] = 1.0\n",
        "            aromaticity = 1.0 if atom.GetIsAromatic() else 0.0\n",
        "            hydrogens = [0.0] * 5\n",
        "            hydrogens[atom.GetTotalNumHs()] = 1.0\n",
        "            chirality = 1.0 if atom.HasProp(\"_ChiralityPossible\") else 0.0\n",
        "            chirality_type = [0.0] * 2\n",
        "            if atom.HasProp(\"_CIPCode\"):\n",
        "                chirality_type[[\"R\", \"S\"].index(atom.GetProp(\"_CIPCode\"))] = 1.0\n",
        "\n",
        "            x = torch.tensor(\n",
        "                symbol\n",
        "                + degree\n",
        "                + [formal_charge]\n",
        "                + [radical_electrons]\n",
        "                + hybridization\n",
        "                + [aromaticity]\n",
        "                + hydrogens\n",
        "                + [chirality]\n",
        "                + chirality_type\n",
        "            )\n",
        "            xs.append(x)\n",
        "\n",
        "        data.x = torch.stack(xs, dim=0)\n",
        "\n",
        "        edge_indices = []\n",
        "        edge_attrs = []\n",
        "        for bond in mol.GetBonds():\n",
        "            edge_indices += [[bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()]]\n",
        "            edge_indices += [[bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()]]\n",
        "\n",
        "            bond_type = bond.GetBondType()\n",
        "            single = 1.0 if bond_type == Chem.rdchem.BondType.SINGLE else 0.0\n",
        "            double = 1.0 if bond_type == Chem.rdchem.BondType.DOUBLE else 0.0\n",
        "            triple = 1.0 if bond_type == Chem.rdchem.BondType.TRIPLE else 0.0\n",
        "            aromatic = 1.0 if bond_type == Chem.rdchem.BondType.AROMATIC else 0.0\n",
        "            conjugation = 1.0 if bond.GetIsConjugated() else 0.0\n",
        "            ring = 1.0 if bond.IsInRing() else 0.0\n",
        "            stereo = [0.0] * 4\n",
        "            stereo[self.stereos.index(bond.GetStereo())] = 1.0\n",
        "\n",
        "            edge_attr = torch.tensor(\n",
        "                [single, double, triple, aromatic, conjugation, ring] + stereo\n",
        "            )\n",
        "\n",
        "            edge_attrs += [edge_attr, edge_attr]\n",
        "\n",
        "        if len(edge_attrs) == 0:\n",
        "            data.edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
        "            data.edge_attr = torch.zeros((0, 10), dtype=torch.float)\n",
        "        else:\n",
        "            data.edge_index = torch.tensor(edge_indices).t().contiguous()\n",
        "            data.edge_attr = torch.stack(edge_attrs, dim=0)\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "path = osp.join(\"data\", \"AFP_Mol\")\n",
        "dataset = MoleculeNet(path, name=\"ESOL\", pre_transform=GenFeatures()).shuffle()\n",
        "\n",
        "N = len(dataset) // 10\n",
        "val_dataset = dataset[:N]\n",
        "test_dataset = dataset[N : 2 * N]\n",
        "train_dataset = dataset[2 * N :]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=200)\n",
        "test_loader = DataLoader(test_dataset, batch_size=200)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AttentiveFP(\n",
        "    in_channels=39,\n",
        "    hidden_channels=200,\n",
        "    out_channels=1,\n",
        "    edge_dim=10,\n",
        "    num_layers=2,\n",
        "    num_timesteps=2,\n",
        "    dropout=0.2,\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=10**-2.5, weight_decay=10**-5)\n",
        "\n",
        "\n",
        "def train():\n",
        "    total_loss = total_examples = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = F.mse_loss(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        total_examples += data.num_graphs\n",
        "    return sqrt(total_loss / total_examples)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    mse = []\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        mse.append(F.mse_loss(out, data.y, reduction=\"none\").cpu())\n",
        "    return float(torch.cat(mse, dim=0).mean().sqrt())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "yh0y8Ia-e3lQ",
        "outputId": "9f091edd-9bc1-4575-e01a-f40644ac0a65"
      },
      "outputs": [],
      "source": [
        "for epoch in (pbar := trange(1, 500, ncols=100)):\n",
        "    train_rmse = train()\n",
        "    val_rmse = test(val_loader)\n",
        "    test_rmse = test(test_loader)\n",
        "    pbar.set_postfix({\"train\": train_rmse, \"val\": val_rmse, \"test\": test_rmse})\n",
        "    # print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} Val: {val_rmse:.4f} '\n",
        "    #       f'Test: {test_rmse:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
