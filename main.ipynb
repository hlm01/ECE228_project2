{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c381f9d8",
   "metadata": {
    "id": "c381f9d8"
   },
   "source": [
    "# attentive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "AA2mdfzmzhx8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AA2mdfzmzhx8",
    "outputId": "0fb393a3-383d-4760-9eb8-48e4124f8304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch_geometric in /home/osathe/.local/lib/python3.11/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.9.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch_geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import GRUCell, Linear, Parameter\n",
    "\n",
    "from torch_geometric.nn import GATConv, MessagePassing, global_add_pool\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "\n",
    "class GATEConv(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        edge_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__(aggr='add', node_dim=0)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.att_l = Parameter(torch.empty(1, out_channels))\n",
    "        self.att_r = Parameter(torch.empty(1, in_channels))\n",
    "\n",
    "        self.lin1 = Linear(in_channels + edge_dim, out_channels, False)\n",
    "        self.lin2 = Linear(out_channels, out_channels, False)\n",
    "\n",
    "        self.bias = Parameter(torch.empty(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.att_l)\n",
    "        glorot(self.att_r)\n",
    "        glorot(self.lin1.weight)\n",
    "        glorot(self.lin2.weight)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: Tensor) -> Tensor:\n",
    "        # edge_updater_type: (x: Tensor, edge_attr: Tensor)\n",
    "        alpha = self.edge_updater(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "        # propagate_type: (x: Tensor, alpha: Tensor)\n",
    "        out = self.propagate(edge_index, x=x, alpha=alpha)\n",
    "        out = out + self.bias\n",
    "        return out\n",
    "\n",
    "    def edge_update(self, x_j: Tensor, x_i: Tensor, edge_attr: Tensor,\n",
    "                    index: Tensor, ptr: OptTensor,\n",
    "                    size_i: Optional[int]) -> Tensor:\n",
    "        x_j = F.leaky_relu_(self.lin1(torch.cat([x_j, edge_attr], dim=-1)))\n",
    "        alpha_j = (x_j @ self.att_l.t()).squeeze(-1)\n",
    "        alpha_i = (x_i @ self.att_r.t()).squeeze(-1)\n",
    "        alpha = alpha_j + alpha_i\n",
    "        alpha = F.leaky_relu_(alpha)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return alpha\n",
    "\n",
    "    def message(self, x_j: Tensor, alpha: Tensor) -> Tensor:\n",
    "        return self.lin2(x_j) * alpha.unsqueeze(-1)\n",
    "\n",
    "\n",
    "class AttentiveFP(torch.nn.Module):\n",
    "    r\"\"\"The Attentive FP model for molecular representation learning from the\n",
    "    `\"Pushing the Boundaries of Molecular Representation for Drug Discovery\n",
    "    with the Graph Attention Mechanism\"\n",
    "    <https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959>`_ paper, based on\n",
    "    graph attention mechanisms.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        hidden_channels (int): Hidden node feature dimensionality.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        edge_dim (int): Edge feature dimensionality.\n",
    "        num_layers (int): Number of GNN layers.\n",
    "        num_timesteps (int): Number of iterative refinement steps for global\n",
    "            readout.\n",
    "        dropout (float, optional): Dropout probability. (default: :obj:`0.0`)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        edge_dim: int,\n",
    "        num_layers: int,\n",
    "        num_timesteps: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.edge_dim = edge_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin1 = Linear(in_channels, hidden_channels)\n",
    "\n",
    "        self.gate_conv = GATEConv(hidden_channels, hidden_channels, edge_dim,\n",
    "                                  dropout)\n",
    "        self.gru = GRUCell(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.atom_convs = torch.nn.ModuleList()\n",
    "        self.atom_grus = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            conv = GATConv(hidden_channels, hidden_channels, dropout=dropout,\n",
    "                           add_self_loops=False, negative_slope=0.01)\n",
    "            self.atom_convs.append(conv)\n",
    "            self.atom_grus.append(GRUCell(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.mol_conv = GATConv(hidden_channels, hidden_channels,\n",
    "                                dropout=dropout, add_self_loops=False,\n",
    "                                negative_slope=0.01)\n",
    "        self.mol_conv.explain = False  # Cannot explain global pooling.\n",
    "        self.mol_gru = GRUCell(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.lin2 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        self.lin1.reset_parameters()\n",
    "        self.gate_conv.reset_parameters()\n",
    "        self.gru.reset_parameters()\n",
    "        for conv, gru in zip(self.atom_convs, self.atom_grus):\n",
    "            conv.reset_parameters()\n",
    "            gru.reset_parameters()\n",
    "        self.mol_conv.reset_parameters()\n",
    "        self.mol_gru.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor,\n",
    "                batch: Tensor) -> Tensor:\n",
    "        \"\"\"\"\"\"  # noqa: D419\n",
    "        # Atom Embedding:\n",
    "        x = F.leaky_relu_(self.lin1(x))\n",
    "\n",
    "        h = F.elu_(self.gate_conv(x, edge_index, edge_attr))\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        x = self.gru(h, x).relu_()\n",
    "\n",
    "        for conv, gru in zip(self.atom_convs, self.atom_grus):\n",
    "            h = conv(x, edge_index)\n",
    "            h = F.elu(h)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            x = gru(h, x).relu()\n",
    "\n",
    "        # Molecule Embedding:\n",
    "        row = torch.arange(batch.size(0), device=batch.device)\n",
    "        edge_index = torch.stack([row, batch], dim=0)\n",
    "\n",
    "        out = global_add_pool(x, batch).relu_()\n",
    "        for t in range(self.num_timesteps):\n",
    "            h = F.elu_(self.mol_conv((x, out), edge_index))\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            out = self.mol_gru(h, out).relu_()\n",
    "\n",
    "        # Predictor:\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        return self.lin2(out)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}('\n",
    "                f'in_channels={self.in_channels}, '\n",
    "                f'hidden_channels={self.hidden_channels}, '\n",
    "                f'out_channels={self.out_channels}, '\n",
    "                f'edge_dim={self.edge_dim}, '\n",
    "                f'num_layers={self.num_layers}, '\n",
    "                f'num_timesteps={self.num_timesteps}'\n",
    "                f')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d2fb5",
   "metadata": {
    "id": "5b6d2fb5"
   },
   "source": [
    "# attentive_fp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9feef81e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9feef81e",
    "outputId": "6d293d31-b149-4be8-9522-123a9fdb76d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rdkit in /home/osathe/.local/lib/python3.11/site-packages (2025.3.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from rdkit) (10.4.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optuna in /home/osathe/.local/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: colorlog in /home/osathe/.local/lib/python3.11/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (24.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna) (2.0.29)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit\n",
    "!pip install optuna\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import os.path as osp\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "#from attentive_fp import AttentiveFP\n",
    "\n",
    "\n",
    "class GenFeatures:\n",
    "    def __init__(self):\n",
    "        self.symbols = [\n",
    "            'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br',\n",
    "            'Te', 'I', 'At', 'other'\n",
    "        ]\n",
    "\n",
    "        self.hybridizations = [\n",
    "            Chem.rdchem.HybridizationType.SP,\n",
    "            Chem.rdchem.HybridizationType.SP2,\n",
    "            Chem.rdchem.HybridizationType.SP3,\n",
    "            Chem.rdchem.HybridizationType.SP3D,\n",
    "            Chem.rdchem.HybridizationType.SP3D2,\n",
    "            'other',\n",
    "        ]\n",
    "\n",
    "        self.stereos = [\n",
    "            Chem.rdchem.BondStereo.STEREONONE,\n",
    "            Chem.rdchem.BondStereo.STEREOANY,\n",
    "            Chem.rdchem.BondStereo.STEREOZ,\n",
    "            Chem.rdchem.BondStereo.STEREOE,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Generate AttentiveFP features according to Table 1.\n",
    "        mol = Chem.MolFromSmiles(data.smiles)\n",
    "\n",
    "        xs = []\n",
    "        from rdkit.Chem import Descriptors\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Generate AttentiveFP features according to Table 1.\n",
    "        mol = Chem.MolFromSmiles(data.smiles)\n",
    "\n",
    "        xs = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            symbol = [0.] * len(self.symbols)\n",
    "            symbol[self.symbols.index(atom.GetSymbol())] = 1.\n",
    "            degree = [0.] * 6\n",
    "            degree[atom.GetDegree()] = 1.\n",
    "            formal_charge = atom.GetFormalCharge()\n",
    "            radical_electrons = atom.GetNumRadicalElectrons()\n",
    "            hybridization = [0.] * len(self.hybridizations)\n",
    "            hybridization[self.hybridizations.index(\n",
    "                atom.GetHybridization())] = 1.\n",
    "            aromaticity = 1. if atom.GetIsAromatic() else 0.\n",
    "            hydrogens = [0.] * 5\n",
    "            hydrogens[atom.GetTotalNumHs()] = 1.\n",
    "            chirality = 1. if atom.HasProp('_ChiralityPossible') else 0.\n",
    "            chirality_type = [0.] * 2\n",
    "            is_in_ring = 1. if atom.IsInRing() else 0.\n",
    "            mass = atom.GetMass()\n",
    "            is_donor = 1. if rdMolDescriptors.CalcNumHBD(mol) > 0 else 0.\n",
    "            is_acceptor = 1. if rdMolDescriptors.CalcNumHBA(mol) > 0 else 0.\n",
    "            if atom.HasProp('_CIPCode'):\n",
    "                chirality_type[['R', 'S'].index(atom.GetProp('_CIPCode'))] = 1.\n",
    "\n",
    "            # Append extra atom features\n",
    "            x = torch.cat((x, torch.tensor([is_in_ring, mass, is_donor, is_acceptor])))\n",
    "\n",
    "            x = torch.tensor(symbol + degree + [formal_charge] +\n",
    "                             [radical_electrons] + hybridization +\n",
    "                             [aromaticity] + hydrogens + [chirality] +\n",
    "                             chirality_type)\n",
    "            xs.append(x)\n",
    "\n",
    "        data.x = torch.stack(xs, dim=0)\n",
    "\n",
    "        edge_indices = []\n",
    "        edge_attrs = []\n",
    "        for bond in mol.GetBonds():\n",
    "            bond_dir = 0.\n",
    "            if bond.GetBondDir() in [BondDir.ENDUPRIGHT, BondDir.ENDDOWNRIGHT]:\n",
    "                bond_dir = 1.\n",
    "\n",
    "            if conf is not None:\n",
    "                idx1, idx2 = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                pos1 = conf.GetAtomPosition(idx1)\n",
    "                pos2 = conf.GetAtomPosition(idx2)\n",
    "                bond_length = float((pos1 - pos2).Length())\n",
    "            else:\n",
    "                bond_length = 0.\n",
    "            edge_indices += [[bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()]]\n",
    "            edge_indices += [[bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()]]\n",
    "\n",
    "            bond_type = bond.GetBondType()\n",
    "            single = 1. if bond_type == Chem.rdchem.BondType.SINGLE else 0.\n",
    "            double = 1. if bond_type == Chem.rdchem.BondType.DOUBLE else 0.\n",
    "            triple = 1. if bond_type == Chem.rdchem.BondType.TRIPLE else 0.\n",
    "            aromatic = 1. if bond_type == Chem.rdchem.BondType.AROMATIC else 0.\n",
    "            conjugation = 1. if bond.GetIsConjugated() else 0.\n",
    "            ring = 1. if bond.IsInRing() else 0.\n",
    "            stereo = [0.] * 4\n",
    "            stereo[self.stereos.index(bond.GetStereo())] = 1.\n",
    "\n",
    "            edge_attr = torch.tensor([bond_length, bond_dir] +\n",
    "                [single, double, triple, aromatic, conjugation] + stereo)\n",
    "\n",
    "            edge_attrs += [edge_attr, edge_attr]\n",
    "\n",
    "        if len(edge_attrs) == 0:\n",
    "            data.edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "            data.edge_attr = torch.zeros((0, 10), dtype=torch.float)\n",
    "        else:\n",
    "            data.edge_index = torch.tensor(edge_indices).t().contiguous()\n",
    "            data.edge_attr = torch.stack(edge_attrs, dim=0)\n",
    "\n",
    "        return data\n",
    "\n",
    "def train():\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = F.mse_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "        total_examples += data.num_graphs\n",
    "    return sqrt(total_loss / total_examples)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    mse = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        mse.append(F.mse_loss(out, data.y, reduction='none').cpu())\n",
    "    return float(torch.cat(mse, dim=0).mean().sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5BAd4goU08Fz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BAd4goU08Fz",
    "outputId": "4acdff6b-b1fd-4964-ceed-8616fc40de51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 1.1568 Val: 1.1498 Test: 1.1486\n",
      "Epoch: 020, Loss: 0.8902 Val: 0.8492 Test: 0.8891\n",
      "Epoch: 030, Loss: 0.8121 Val: 0.7784 Test: 0.8777\n",
      "Epoch: 040, Loss: 0.7622 Val: 0.7534 Test: 0.7831\n",
      "Epoch: 050, Loss: 0.7305 Val: 0.7528 Test: 0.7221\n",
      "Epoch: 060, Loss: 0.6625 Val: 0.6728 Test: 0.7889\n",
      "Epoch: 070, Loss: 0.6547 Val: 0.7016 Test: 0.7414\n",
      "Epoch: 080, Loss: 0.6312 Val: 0.6972 Test: 0.6891\n",
      "Epoch: 090, Loss: 0.6103 Val: 0.6567 Test: 0.6414\n",
      "Epoch: 100, Loss: 0.5933 Val: 0.6645 Test: 0.6653\n",
      "Epoch: 110, Loss: 0.5614 Val: 0.6585 Test: 0.7025\n",
      "Epoch: 120, Loss: 0.5637 Val: 0.6277 Test: 0.6091\n",
      "Epoch: 130, Loss: 0.5553 Val: 0.6233 Test: 0.6273\n",
      "Epoch: 140, Loss: 0.5498 Val: 0.6414 Test: 0.6698\n",
      "Epoch: 150, Loss: 0.5177 Val: 0.6127 Test: 0.6372\n",
      "Epoch: 160, Loss: 0.5011 Val: 0.5924 Test: 0.6420\n",
      "Epoch: 170, Loss: 0.4980 Val: 0.5505 Test: 0.6035\n",
      "Epoch: 180, Loss: 0.4833 Val: 0.5764 Test: 0.5724\n",
      "Epoch: 190, Loss: 0.4614 Val: 0.5517 Test: 0.5809\n",
      "Epoch: 200, Loss: 0.5016 Val: 0.5645 Test: 0.5713\n",
      "Epoch: 210, Loss: 0.4486 Val: 0.5763 Test: 0.5961\n",
      "Epoch: 220, Loss: 0.4554 Val: 0.5443 Test: 0.5777\n",
      "Epoch: 230, Loss: 0.4636 Val: 0.6298 Test: 0.5329\n",
      "Epoch: 240, Loss: 0.4342 Val: 0.6093 Test: 0.5672\n",
      "Epoch: 250, Loss: 0.4718 Val: 0.5999 Test: 0.5610\n",
      "Epoch: 260, Loss: 0.4287 Val: 0.5260 Test: 0.6013\n",
      "Epoch: 270, Loss: 0.4253 Val: 0.5969 Test: 0.5962\n",
      "Epoch: 280, Loss: 0.4163 Val: 0.5833 Test: 0.5568\n",
      "Epoch: 290, Loss: 0.4101 Val: 0.6127 Test: 0.5897\n",
      "Epoch: 300, Loss: 0.4092 Val: 0.6515 Test: 0.5911\n",
      "Epoch: 310, Loss: 0.4042 Val: 0.5983 Test: 0.5858\n",
      "Epoch: 320, Loss: 0.3953 Val: 0.6094 Test: 0.6057\n",
      "Epoch: 330, Loss: 0.3889 Val: 0.6608 Test: 0.5897\n",
      "Epoch: 340, Loss: 0.3877 Val: 0.6497 Test: 0.5447\n",
      "Epoch: 350, Loss: 0.3960 Val: 0.6471 Test: 0.5261\n",
      "Epoch: 360, Loss: 0.3636 Val: 0.5943 Test: 0.5582\n",
      "Epoch: 370, Loss: 0.3780 Val: 0.5856 Test: 0.5577\n",
      "Epoch: 380, Loss: 0.3567 Val: 0.6129 Test: 0.5468\n",
      "Epoch: 390, Loss: 0.3653 Val: 0.6307 Test: 0.5651\n",
      "Epoch: 400, Loss: 0.3557 Val: 0.6405 Test: 0.6258\n",
      "Epoch: 410, Loss: 0.3702 Val: 0.5976 Test: 0.5299\n",
      "Epoch: 420, Loss: 0.3486 Val: 0.6734 Test: 0.5036\n",
      "Epoch: 430, Loss: 0.3337 Val: 0.6827 Test: 0.5606\n",
      "Epoch: 440, Loss: 0.3565 Val: 0.6448 Test: 0.5604\n",
      "Epoch: 450, Loss: 0.3434 Val: 0.6902 Test: 0.5946\n",
      "Epoch: 460, Loss: 0.3261 Val: 0.6656 Test: 0.5833\n",
      "Epoch: 470, Loss: 0.3581 Val: 0.6042 Test: 0.5836\n",
      "Epoch: 480, Loss: 0.3590 Val: 0.5607 Test: 0.5738\n",
      "Epoch: 490, Loss: 0.3625 Val: 0.6547 Test: 0.5832\n",
      "Epoch: 500, Loss: 0.3349 Val: 0.5724 Test: 0.5326\n",
      "Epoch: 510, Loss: 0.3463 Val: 0.6292 Test: 0.5753\n",
      "Epoch: 520, Loss: 0.3205 Val: 0.6382 Test: 0.5528\n",
      "Epoch: 530, Loss: 0.3742 Val: 0.5652 Test: 0.5198\n",
      "Epoch: 540, Loss: 0.3148 Val: 0.6524 Test: 0.5353\n",
      "Epoch: 550, Loss: 0.3555 Val: 0.6289 Test: 0.5586\n",
      "Epoch: 560, Loss: 0.3175 Val: 0.6017 Test: 0.5889\n",
      "Epoch: 570, Loss: 0.3263 Val: 0.6942 Test: 0.4988\n",
      "Epoch: 580, Loss: 0.3109 Val: 0.6798 Test: 0.5428\n",
      "Epoch: 590, Loss: 0.3059 Val: 0.6204 Test: 0.5571\n",
      "Epoch: 600, Loss: 0.3245 Val: 0.6353 Test: 0.5558\n",
      "Epoch: 610, Loss: 0.3092 Val: 0.6359 Test: 0.5660\n",
      "Epoch: 620, Loss: 0.3158 Val: 0.6112 Test: 0.5339\n",
      "Epoch: 630, Loss: 0.3161 Val: 0.5774 Test: 0.5898\n",
      "Epoch: 640, Loss: 0.3078 Val: 0.6582 Test: 0.5979\n",
      "Epoch: 650, Loss: 0.3229 Val: 0.6358 Test: 0.5428\n",
      "Epoch: 660, Loss: 0.2990 Val: 0.6195 Test: 0.5642\n",
      "Epoch: 670, Loss: 0.3111 Val: 0.5752 Test: 0.5083\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import os.path as osp\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.rdchem import BondDir\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Compute molecule-level info once\n",
    "#mol = Chem.AddHs(mol)\n",
    "#AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "#conf = mol.GetConformer() if mol.GetNumConformers() > 0 else None\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(21)\n",
    "\n",
    "# Use current working directory in Jupyter\n",
    "notebook_dir = osp.dirname(osp.abspath(''))\n",
    "path = osp.abspath('../data/AFP_Mol')\n",
    "\n",
    "#path = '/content/drive/MyDrive/my_project/data/AFP_Mol'\n",
    "dataset = MoleculeNet(path, name='ESOL', pre_transform=GenFeatures()).shuffle()\n",
    "\n",
    "N = len(dataset) // 10\n",
    "val_dataset = dataset[:N]\n",
    "test_dataset = dataset[N:2 * N]\n",
    "train_dataset = dataset[2 * N:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=200)\n",
    "test_loader = DataLoader(test_dataset, batch_size=200)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AttentiveFP(in_channels=39, hidden_channels=250, out_channels=1,\n",
    "                    edge_dim=10, num_layers=3, num_timesteps=2,\n",
    "                    dropout=0.2).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003,\n",
    "                             weight_decay=0.00001)\n",
    "\n",
    "for epoch in range(1, 671):\n",
    "    train_rmse = train()\n",
    "    val_rmse = test(val_loader)\n",
    "    test_rmse = test(test_loader)\n",
    "    if(epoch%10==0):\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} Val: {val_rmse:.4f} '\n",
    "          f'Test: {test_rmse:.4f}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    set_seed(20)  # Ensures reproducibility\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "\n",
    "    # Define model and optimizer with trial parameters\n",
    "    model = AttentiveFP(\n",
    "        in_channels=39,\n",
    "        hidden_channels=200,\n",
    "        out_channels=1,\n",
    "        edge_dim=10,\n",
    "        num_layers=num_layers,\n",
    "        num_timesteps=2,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    best_val = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = 20\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        train_rmse = train()\n",
    "        val_rmse = test(test_loader)\n",
    "\n",
    "        if val_rmse < best_val:\n",
    "            best_val = val_rmse\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Save best model for this trial\n",
    "    trial.set_user_attr(\"best_model_state_dict\", best_model_state)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "# Run study\n",
    "#study = optuna.create_study(direction=\"minimize\")\n",
    "#study.optimize(objective, n_trials=25)\n",
    "\n",
    "# Report best trial\n",
    "#best_trial = study.best_trial\n",
    "#print(\"Best Trial:\")\n",
    "#print(f\"  Value: {best_trial.value:.4f}\")\n",
    "#for k, v in best_trial.params.items():\n",
    "#    print(f\"  {k}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78d9633c-d4c2-4e62-8993-102424fdfad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 2.9087 Val: 2.7666 Test: 2.4191\n",
      "Epoch: 020, Loss: 1.6465 Val: 1.7319 Test: 1.6867\n",
      "Epoch: 030, Loss: 1.2699 Val: 1.3590 Test: 1.2912\n",
      "Epoch: 040, Loss: 1.1079 Val: 1.1130 Test: 1.3638\n",
      "Epoch: 050, Loss: 1.0673 Val: 1.1454 Test: 1.3659\n",
      "Epoch: 060, Loss: 0.9814 Val: 1.2775 Test: 1.4030\n",
      "Epoch: 070, Loss: 0.9375 Val: 1.0887 Test: 1.2712\n",
      "Epoch: 080, Loss: 0.8385 Val: 1.2452 Test: 1.1866\n",
      "Epoch: 090, Loss: 0.9781 Val: 1.0845 Test: 1.2516\n",
      "Epoch: 100, Loss: 0.8011 Val: 1.0479 Test: 1.1853\n",
      "Epoch: 110, Loss: 0.7509 Val: 1.0340 Test: 1.2274\n",
      "Epoch: 120, Loss: 0.8075 Val: 1.0576 Test: 1.1060\n",
      "Epoch: 130, Loss: 0.6985 Val: 1.0365 Test: 1.0541\n",
      "Epoch: 140, Loss: 0.7196 Val: 0.9471 Test: 1.2061\n",
      "Epoch: 150, Loss: 0.7139 Val: 1.1354 Test: 1.0715\n",
      "Epoch: 160, Loss: 0.6619 Val: 0.9191 Test: 1.0560\n",
      "Epoch: 170, Loss: 0.5961 Val: 1.0185 Test: 1.1495\n",
      "Epoch: 180, Loss: 0.6220 Val: 1.1894 Test: 1.0884\n",
      "Epoch: 190, Loss: 0.6946 Val: 1.0160 Test: 1.1435\n",
      "Epoch: 200, Loss: 0.6005 Val: 0.9741 Test: 1.0351\n",
      "Epoch: 210, Loss: 0.6308 Val: 1.1705 Test: 0.9759\n",
      "Epoch: 220, Loss: 0.5852 Val: 0.9531 Test: 0.9476\n",
      "Epoch: 230, Loss: 0.5509 Val: 1.0970 Test: 0.8414\n",
      "Epoch: 240, Loss: 0.5548 Val: 0.8953 Test: 0.9726\n",
      "Epoch: 250, Loss: 0.6481 Val: 1.1936 Test: 1.0331\n",
      "Epoch: 260, Loss: 0.5345 Val: 1.0237 Test: 1.0305\n",
      "Epoch: 270, Loss: 0.5681 Val: 1.1259 Test: 1.0163\n",
      "Epoch: 280, Loss: 0.6212 Val: 1.1019 Test: 1.0207\n",
      "Epoch: 290, Loss: 0.5611 Val: 1.0431 Test: 0.9441\n",
      "Epoch: 300, Loss: 0.5114 Val: 1.0046 Test: 0.8743\n",
      "Epoch: 310, Loss: 0.5236 Val: 1.1248 Test: 0.9351\n",
      "Epoch: 320, Loss: 0.4848 Val: 1.1759 Test: 0.8680\n",
      "Epoch: 330, Loss: 0.4582 Val: 1.0744 Test: 0.8887\n",
      "Epoch: 340, Loss: 0.5247 Val: 1.1276 Test: 0.8055\n",
      "Epoch: 350, Loss: 0.4605 Val: 1.0570 Test: 0.9637\n",
      "Epoch: 360, Loss: 0.4793 Val: 1.0785 Test: 0.9056\n",
      "Epoch: 370, Loss: 0.5010 Val: 1.0249 Test: 0.9195\n",
      "Epoch: 380, Loss: 0.4467 Val: 0.9895 Test: 1.0179\n",
      "Epoch: 390, Loss: 0.4602 Val: 0.9869 Test: 0.8613\n",
      "Epoch: 400, Loss: 0.4757 Val: 1.0592 Test: 0.8885\n",
      "Epoch: 410, Loss: 0.4284 Val: 0.9592 Test: 0.8746\n",
      "Epoch: 420, Loss: 0.4493 Val: 1.0521 Test: 0.8424\n",
      "Epoch: 430, Loss: 0.3902 Val: 1.0799 Test: 0.9056\n",
      "Epoch: 440, Loss: 0.4103 Val: 0.9846 Test: 0.8182\n",
      "Epoch: 450, Loss: 0.4487 Val: 0.9709 Test: 0.8608\n",
      "Epoch: 460, Loss: 0.4469 Val: 0.9554 Test: 0.8567\n",
      "Epoch: 470, Loss: 0.4401 Val: 0.9442 Test: 0.8015\n",
      "Epoch: 480, Loss: 0.4143 Val: 0.9352 Test: 0.8232\n",
      "Epoch: 490, Loss: 0.3993 Val: 1.0547 Test: 0.7987\n",
      "Epoch: 500, Loss: 0.4880 Val: 1.0128 Test: 0.8967\n",
      "Epoch: 510, Loss: 0.3951 Val: 0.9967 Test: 0.8086\n",
      "Epoch: 520, Loss: 0.4025 Val: 0.9400 Test: 0.8417\n",
      "Epoch: 530, Loss: 0.4038 Val: 0.8694 Test: 0.7858\n",
      "Epoch: 540, Loss: 0.3645 Val: 1.0541 Test: 0.8576\n",
      "Epoch: 550, Loss: 0.3868 Val: 1.0672 Test: 0.7659\n",
      "Epoch: 560, Loss: 0.4107 Val: 0.9404 Test: 0.8598\n",
      "Epoch: 570, Loss: 0.3836 Val: 1.1121 Test: 0.8156\n",
      "Epoch: 580, Loss: 0.3659 Val: 0.8894 Test: 0.8646\n",
      "Epoch: 590, Loss: 0.3713 Val: 0.9600 Test: 0.7967\n",
      "Epoch: 600, Loss: 0.3476 Val: 0.9907 Test: 0.8293\n",
      "Epoch: 610, Loss: 0.3587 Val: 0.9641 Test: 0.8509\n",
      "Epoch: 620, Loss: 0.3773 Val: 0.9537 Test: 0.7921\n",
      "Epoch: 630, Loss: 0.3951 Val: 0.9430 Test: 0.8332\n",
      "Epoch: 640, Loss: 0.3822 Val: 1.0934 Test: 0.8298\n",
      "Epoch: 650, Loss: 0.3604 Val: 0.9318 Test: 0.8389\n",
      "Epoch: 660, Loss: 0.3434 Val: 0.9836 Test: 0.7923\n",
      "Epoch: 670, Loss: 0.3422 Val: 0.9953 Test: 0.7449\n",
      "Epoch: 680, Loss: 0.3682 Val: 0.9376 Test: 0.8440\n",
      "Epoch: 690, Loss: 0.3370 Val: 0.8281 Test: 0.7204\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import os.path as osp\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(21)\n",
    "\n",
    "# Use current working directory in Jupyter\n",
    "notebook_dir = osp.dirname(osp.abspath(''))\n",
    "path = osp.abspath('../data/AFP_Mol')\n",
    "\n",
    "#path = '/content/drive/MyDrive/my_project/data/AFP_Mol'\n",
    "dataset = MoleculeNet(path, name='FreeSolv', pre_transform=GenFeatures()).shuffle()\n",
    "\n",
    "N = len(dataset) // 10\n",
    "val_dataset = dataset[:N]\n",
    "test_dataset = dataset[N:2 * N]\n",
    "train_dataset = dataset[2 * N:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=200)\n",
    "test_loader = DataLoader(test_dataset, batch_size=200)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AttentiveFP(in_channels=39, hidden_channels=250, out_channels=1,\n",
    "                    edge_dim=10, num_layers=4, num_timesteps=2,\n",
    "                    dropout=0.2).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001329477429508522,\n",
    "                             weight_decay=1.8804404706749282e-05)\n",
    "\n",
    "for epoch in range(1, 691):\n",
    "    train_rmse = train()\n",
    "    val_rmse = test(val_loader)\n",
    "    test_rmse = test(test_loader)\n",
    "    if(epoch%10==0):\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} Val: {val_rmse:.4f} '\n",
    "          f'Test: {test_rmse:.4f}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    set_seed(21)  # Ensures reproducibility\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "\n",
    "    # Define model and optimizer with trial parameters\n",
    "    model = AttentiveFP(\n",
    "        in_channels=39,\n",
    "        hidden_channels=200,\n",
    "        out_channels=1,\n",
    "        edge_dim=10,\n",
    "        num_layers=num_layers,\n",
    "        num_timesteps=2,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    best_val = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = 20\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        train_rmse = train()\n",
    "        val_rmse = test(test_loader)\n",
    "\n",
    "        if val_rmse < best_val:\n",
    "            best_val = val_rmse\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Save best model for this trial\n",
    "    trial.set_user_attr(\"best_model_state_dict\", best_model_state)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "# Run study\n",
    "#study = optuna.create_study(direction=\"minimize\")\n",
    "#study.optimize(objective, n_trials=25)\n",
    "\n",
    "# Report best trial\n",
    "#best_trial = study.best_trial\n",
    "#print(\"Best Trial:\")\n",
    "#print(f\"  Value: {best_trial.value:.4f}\")\n",
    "#for k, v in best_trial.params.items():\n",
    "#    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4dd34c8-c7b4-4a7f-b4b8-d4080eb74576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.4976 Val: 0.4935 Test: 0.4994\n",
      "Epoch: 020, Loss: 0.4850 Val: 0.4829 Test: 0.4988\n",
      "Epoch: 030, Loss: 0.4412 Val: 0.4469 Test: 0.4618\n",
      "Epoch: 040, Loss: 0.4382 Val: 0.4256 Test: 0.4594\n",
      "Epoch: 050, Loss: 0.4226 Val: 0.4298 Test: 0.4424\n",
      "Epoch: 060, Loss: 0.4227 Val: 0.4443 Test: 0.4554\n",
      "Epoch: 070, Loss: 0.4257 Val: 0.4354 Test: 0.4317\n",
      "Epoch: 080, Loss: 0.4109 Val: 0.4157 Test: 0.4257\n",
      "Epoch: 090, Loss: 0.4055 Val: 0.4326 Test: 0.4298\n",
      "Epoch: 100, Loss: 0.4058 Val: 0.4620 Test: 0.4326\n",
      "Epoch: 110, Loss: 0.3822 Val: 0.4628 Test: 0.4447\n",
      "Epoch: 120, Loss: 0.3826 Val: 0.4043 Test: 0.4152\n",
      "Epoch: 130, Loss: 0.3996 Val: 0.4430 Test: 0.4123\n",
      "Epoch: 140, Loss: 0.4001 Val: 0.4330 Test: 0.4051\n",
      "Epoch: 150, Loss: 0.3959 Val: 0.4055 Test: 0.4145\n",
      "Epoch: 160, Loss: 0.3820 Val: 0.4326 Test: 0.4093\n",
      "Epoch: 170, Loss: 0.3911 Val: 0.4403 Test: 0.4288\n",
      "Epoch: 180, Loss: 0.3877 Val: 0.3991 Test: 0.4219\n",
      "Epoch: 190, Loss: 0.3770 Val: 0.4478 Test: 0.4336\n",
      "Epoch: 200, Loss: 0.3701 Val: 0.4213 Test: 0.3997\n",
      "Epoch: 210, Loss: 0.3760 Val: 0.3920 Test: 0.4141\n",
      "Epoch: 220, Loss: 0.3731 Val: 0.4523 Test: 0.4455\n",
      "Epoch: 230, Loss: 0.3750 Val: 0.4193 Test: 0.4127\n",
      "Epoch: 240, Loss: 0.3673 Val: 0.4346 Test: 0.4074\n",
      "Epoch: 250, Loss: 0.3687 Val: 0.4101 Test: 0.3955\n",
      "Epoch: 260, Loss: 0.3628 Val: 0.4139 Test: 0.4074\n",
      "Epoch: 270, Loss: 0.4016 Val: 0.4030 Test: 0.4079\n",
      "Epoch: 280, Loss: 0.3652 Val: 0.4202 Test: 0.3779\n",
      "Epoch: 290, Loss: 0.3654 Val: 0.3993 Test: 0.3990\n",
      "Epoch: 300, Loss: 0.3616 Val: 0.4033 Test: 0.3970\n",
      "Epoch: 310, Loss: 0.3661 Val: 0.4086 Test: 0.3906\n",
      "Epoch: 320, Loss: 0.3794 Val: 0.4258 Test: 0.4067\n",
      "Epoch: 330, Loss: 0.3634 Val: 0.3821 Test: 0.3777\n",
      "Epoch: 340, Loss: 0.3737 Val: 0.4264 Test: 0.3970\n",
      "Epoch: 350, Loss: 0.3903 Val: 0.4280 Test: 0.4115\n",
      "Epoch: 360, Loss: 0.3790 Val: 0.4173 Test: 0.4168\n",
      "Epoch: 370, Loss: 0.3624 Val: 0.4007 Test: 0.3783\n",
      "Epoch: 380, Loss: 0.3774 Val: 0.4022 Test: 0.4101\n",
      "Epoch: 390, Loss: 0.3728 Val: 0.4232 Test: 0.4306\n",
      "Epoch: 400, Loss: 0.3714 Val: 0.4061 Test: 0.3923\n",
      "Epoch: 410, Loss: 0.3836 Val: 0.4447 Test: 0.4343\n",
      "Epoch: 420, Loss: 0.3656 Val: 0.4094 Test: 0.4122\n",
      "Epoch: 430, Loss: 0.3589 Val: 0.4131 Test: 0.4061\n",
      "Epoch: 440, Loss: 0.3627 Val: 0.3858 Test: 0.4008\n",
      "Epoch: 450, Loss: 0.3824 Val: 0.4110 Test: 0.3860\n",
      "Epoch: 460, Loss: 0.3592 Val: 0.4024 Test: 0.3836\n",
      "Epoch: 470, Loss: 0.3947 Val: 0.4659 Test: 0.4343\n",
      "Epoch: 480, Loss: 0.3812 Val: 0.3968 Test: 0.4137\n",
      "Epoch: 490, Loss: 0.3681 Val: 0.4371 Test: 0.3986\n",
      "Epoch: 500, Loss: 0.3801 Val: 0.4145 Test: 0.3940\n",
      "Epoch: 510, Loss: 0.3700 Val: 0.4177 Test: 0.4071\n",
      "Epoch: 520, Loss: 0.3561 Val: 0.4305 Test: 0.4141\n",
      "Epoch: 530, Loss: 0.3459 Val: 0.4332 Test: 0.4008\n",
      "Epoch: 540, Loss: 0.3583 Val: 0.4398 Test: 0.4488\n",
      "Epoch: 550, Loss: 0.3632 Val: 0.4289 Test: 0.4143\n",
      "Epoch: 560, Loss: 0.3668 Val: 0.3769 Test: 0.3725\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import os.path as osp\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(21)\n",
    "\n",
    "# Use current working directory in Jupyter\n",
    "notebook_dir = osp.dirname(osp.abspath(''))\n",
    "path = osp.abspath('../data/AFP_Mol')\n",
    "\n",
    "#path = '/content/drive/MyDrive/my_project/data/AFP_Mol'\n",
    "dataset = MoleculeNet(path, name='BACE', pre_transform=GenFeatures()).shuffle()\n",
    "\n",
    "N = len(dataset) // 10\n",
    "val_dataset = dataset[:N]\n",
    "test_dataset = dataset[N:2 * N]\n",
    "train_dataset = dataset[2 * N:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=200)\n",
    "test_loader = DataLoader(test_dataset, batch_size=200)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AttentiveFP(in_channels=39, hidden_channels=200, out_channels=1,\n",
    "                    edge_dim=10, num_layers=3, num_timesteps=2,\n",
    "                    dropout=0.2).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.008762276062649685,\n",
    "                             weight_decay=1.1650462882147405e-06)\n",
    "\n",
    "for epoch in range(1, 561):\n",
    "    train_rmse = train()\n",
    "    val_rmse = test(val_loader)\n",
    "    test_rmse = test(test_loader)\n",
    "    if(epoch%10==0):\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {train_rmse:.4f} Val: {val_rmse:.4f} '\n",
    "          f'Test: {test_rmse:.4f}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    set_seed(21)  # Ensures reproducibility\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "\n",
    "    # Define model and optimizer with trial parameters\n",
    "    model = AttentiveFP(\n",
    "        in_channels=39,\n",
    "        hidden_channels=200,\n",
    "        out_channels=1,\n",
    "        edge_dim=10,\n",
    "        num_layers=num_layers,\n",
    "        num_timesteps=2,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    best_val = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = 20\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        train_rmse = train()\n",
    "        val_rmse = test(test_loader)\n",
    "\n",
    "        if val_rmse < best_val:\n",
    "            best_val = val_rmse\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Save best model for this trial\n",
    "    trial.set_user_attr(\"best_model_state_dict\", best_model_state)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "# Run study\n",
    "#study = optuna.create_study(direction=\"minimize\")\n",
    "#study.optimize(objective, n_trials=25)\n",
    "\n",
    "# Report best trial\n",
    "#best_trial = study.best_trial\n",
    "#print(\"Best Trial:\")\n",
    "#print(f\"  Value: {best_trial.value:.4f}\")\n",
    "#for k, v in best_trial.params.items():\n",
    "#    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0941bdf-704c-445c-bc4f-dc62276bd2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
